{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, RidgeCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    ShuffleSplit,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    PolynomialFeatures,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering <a name=\"1\"></a>\n",
    "<hr>\n",
    "\n",
    "One of the most important aspects which influences performance of machine learning models is the features used to represent the problem. If your underlying representation is bad whatever fancy model you use is not going to help. With a better feature representation, a simple and a more interpretable model is likely to perform reasonably well. \n",
    "\n",
    "**Feature engineering** is the process of transforming raw data into features that better represent the underlying problem to the predictive models. \n",
    "\n",
    "The code below reads the data CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3289</th>\n",
       "      <td>debris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unfortunately, both plans fail as the 3 are im...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>crash</td>\n",
       "      <td>SLC</td>\n",
       "      <td>I hope this causes Bernie to crash and bern. S...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>collide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>â€”pushes himself up from the chair beneath to r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9622</th>\n",
       "      <td>suicide%20bomb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Widow of CIA agent killed in 2009 Afghanistan ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>screaming</td>\n",
       "      <td>Azania</td>\n",
       "      <td>As soon as God say yes they'll be screaming we...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             keyword location  \\\n",
       "3289          debris      NaN   \n",
       "2672           crash      SLC   \n",
       "2436         collide      NaN   \n",
       "9622  suicide%20bomb      NaN   \n",
       "8999       screaming   Azania   \n",
       "\n",
       "                                                   text  target  \n",
       "3289  Unfortunately, both plans fail as the 3 are im...       0  \n",
       "2672  I hope this causes Bernie to crash and bern. S...       0  \n",
       "2436  â€”pushes himself up from the chair beneath to r...       0  \n",
       "9622  Widow of CIA agent killed in 2009 Afghanistan ...       1  \n",
       "8999  As soon as God say yes they'll be screaming we...       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets.csv\", usecols=[\"keyword\", \"text\", \"target\", \"location\"])\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=2)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_df.drop(columns=[\"target\"]), train_df[\"target\"]\n",
    "X_test, y_test = test_df.drop(columns=[\"target\"]), test_df[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge here is correctly classifying the real disaster tweets. The data set includes tweets about disasters as well as keyword that relate the disaster, such as crash, suicide bomb, and so on and also the location. The prediction problem we're attempting to tackle is determining whether a tweet is connected to a real disaster or is merely a joke/movie review in a disaster-related environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7395\n",
       "1    1701\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, there is a class imbalance in the given data set. As can be seen from the target value counts above, there are only 1701 tweets of genuine disaster, accounting for less than 20% of the whole data set. To cope with this, we'll need to employ a different scoring measure than accuracy, one that focuses on judging the model's performance based on actual disaster tweets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = [\"precision\" , \"f1\", \"recall\", \"roc_auc\" , \"average_precision\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above there is a class imbalance in the given data set.The scoring metric `accuracy` cannot be used to judge the model performance. For the given use case i.e. classifying the actual disaster tweets correctly, we need to be confident that the tweet is actually a disaster, a suitable metric here would be `precision` but we also need to avoid the false negatives which means the `recall` is equally important. So in order to account for the trade off `f1` score sounds a better scoring metric for the given use case. To examine how successfully the model discriminate between the two classes, we utilize the `auc roc` score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The location feature\n",
    "The location feature seems quite messy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'SLC', 'Azania', ..., 'Santiago de Chile', 'she/her ðŸŒˆ',\n",
       "       'Greater Manchester'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"location\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3747"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df[\"location\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9096 entries, 3289 to 7336\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   keyword   9096 non-null   object\n",
      " 1   location  6370 non-null   object\n",
      " 2   text      9096 non-null   object\n",
      " 3   target    9096 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 355.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    6370\n",
       "True     2726\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[\"location\"].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United States                     80\n",
       "Australia                         68\n",
       "London, England                   66\n",
       "UK                                62\n",
       "India                             60\n",
       "                                  ..\n",
       "Arizona City, AZ                   1\n",
       "Yorkshire & Scotland               1\n",
       "th: hakuna matata                  1\n",
       "Tacloban City, Eastern Visayas     1\n",
       "Greater Manchester                 1\n",
       "Name: location, Length: 3746, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[\"location\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) There are many null values in the feature Location i.e. 2726 which might be a challenge to impute.\n",
    "2) Throughout the location feature, there are countries, cities, regions, and other meaningless terms jumbled in. \n",
    "3) There are a lot of instances in the feature location with special characters that aren't in an acceptable format. \n",
    "4) Furthermore, the information is not standardized. \n",
    "5) Because there are 3747 distinct values, applying one hot encoding is difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may divide the location feature into four categories: country, city, region, and other, and then run OHE on each of them. The category \"OTHER\" can be allocated to null and non-meaningful words. But in given case we choose to drop the feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying feature types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9096, 23627)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "drop_features = [\"location\"]\n",
    "text_feature = \"text\"\n",
    "key_word = \"keyword\"\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (CountVectorizer(stop_words=\"english\"), text_feature),\n",
    "    (CountVectorizer(stop_words=\"english\"), key_word)\n",
    ")\n",
    "\n",
    "\n",
    "data=preprocessor.fit_transform(X_train,y_train)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to null values and values that will not contribute significance to the model, we are removing the feature `location.`\n",
    "\n",
    "We use the `Count Vectorizer` feature to turn the `text` feature into vectorized values for each meaningful word that the model can interpret.\n",
    "\n",
    "We use a separate `Count Vectorizer` on the `keyword` feature so that the model may take into account the most common disaster-related keywords when making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        scikit-learn model\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        X in the training data\n",
    "    y_train :\n",
    "        y in the training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        pandas Series with mean scores from cross_validation\n",
    "    \"\"\"\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = make_pipeline(preprocessor, DummyClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/573/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/miniconda3/envs/573/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/miniconda3/envs/573/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/miniconda3/envs/573/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/miniconda3/envs/573/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dummy Classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>0.122 (+/- 0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.053 (+/- 0.002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_roc_auc</th>\n",
       "      <td>0.500 (+/- 0.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_average_precision</th>\n",
       "      <td>0.187 (+/- 0.000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Dummy Classifier\n",
       "fit_time                0.122 (+/- 0.004)\n",
       "score_time              0.053 (+/- 0.002)\n",
       "test_precision          0.000 (+/- 0.000)\n",
       "test_f1                 0.000 (+/- 0.000)\n",
       "test_recall             0.000 (+/- 0.000)\n",
       "test_roc_auc            0.500 (+/- 0.000)\n",
       "test_average_precision  0.187 (+/- 0.000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['Dummy Classifier']= mean_std_cross_val_scores(dummy, X_train, y_train, scoring =scoring)\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('countvectorizer-1',\n",
       "                                                  CountVectorizer(stop_words='english'),\n",
       "                                                  'text'),\n",
       "                                                 ('countvectorizer-2',\n",
       "                                                  CountVectorizer(stop_words='english'),\n",
       "                                                  'keyword')])),\n",
       "                ('logisticregression', LogisticRegression(max_iter=2000))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = make_pipeline(preprocessor, LogisticRegression(max_iter =2000))\n",
    "LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dummy Classifier</th>\n",
       "      <th>Logistic Regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>0.122 (+/- 0.004)</td>\n",
       "      <td>0.301 (+/- 0.016)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.053 (+/- 0.002)</td>\n",
       "      <td>0.058 (+/- 0.002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "      <td>0.811 (+/- 0.012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "      <td>0.628 (+/- 0.026)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "      <td>0.513 (+/- 0.036)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_roc_auc</th>\n",
       "      <td>0.500 (+/- 0.000)</td>\n",
       "      <td>0.898 (+/- 0.011)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_average_precision</th>\n",
       "      <td>0.187 (+/- 0.000)</td>\n",
       "      <td>0.747 (+/- 0.018)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Dummy Classifier Logistic Regression\n",
       "fit_time                0.122 (+/- 0.004)   0.301 (+/- 0.016)\n",
       "score_time              0.053 (+/- 0.002)   0.058 (+/- 0.002)\n",
       "test_precision          0.000 (+/- 0.000)   0.811 (+/- 0.012)\n",
       "test_f1                 0.000 (+/- 0.000)   0.628 (+/- 0.026)\n",
       "test_recall             0.000 (+/- 0.000)   0.513 (+/- 0.036)\n",
       "test_roc_auc            0.500 (+/- 0.000)   0.898 (+/- 0.011)\n",
       "test_average_precision  0.187 (+/- 0.000)   0.747 (+/- 0.018)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['Logistic Regression']= mean_std_cross_val_scores(LR, X_train, y_train, scoring =scoring)\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper-Parameters are: {'logisticregression__class_weight': 'balanced', 'logisticregression__C': 1.0, 'columntransformer__countvectorizer-2__max_features': 50, 'columntransformer__countvectorizer-1__max_features': 15000}\n"
     ]
    }
   ],
   "source": [
    "param_grid_gamma_random = {\"columntransformer__countvectorizer-1__max_features\": [1000,5000,10000,15000,20000, 23000],\n",
    "                           \"columntransformer__countvectorizer-2__max_features\": [50,100,200,219],\n",
    "                           \"logisticregression__C\": 10.0 ** np.arange(-3, 4),\n",
    "                           \"logisticregression__class_weight\": [None, \"balanced\"]}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    LR, param_distributions=param_grid_gamma_random, n_jobs=-1,scoring =\"f1\",random_state =123, n_iter =20\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hyper-Parameters are:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score is: 0.6717594271881064\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Score is:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hyper parameters are: {'logistic regression - class_weight': 'balanced', 'logistic regression- C': 1.0, 'Count Vectorizer-text-max_features': 50, 'Count Vectorizer-keyword-max_features': 15000}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best cross validation f1 score is 0.6717594271881064"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/varadrajrameshpoojary/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/varadrajrameshpoojary/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_length(text, TWITTER_ALLOWED_CHARS=280.0):\n",
    "    \"\"\"\n",
    "    Returns the relative length of text.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------\n",
    "    TWITTER_ALLOWED_CHARS: (float)\n",
    "    the denominator for finding relative length\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    relative length of text: (float)\n",
    "\n",
    "    \"\"\"\n",
    "    return len(text) / TWITTER_ALLOWED_CHARS\n",
    "\n",
    "\n",
    "def get_length_in_words(text):\n",
    "    \"\"\"\n",
    "    Returns the length of the text in words.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    length of tokenized text: (int)\n",
    "\n",
    "    \"\"\"\n",
    "    return len(nltk.word_tokenize(text))\n",
    "\n",
    "\n",
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Returns the compound score representing the sentiment of the given text: -1 (most extreme negative) and +1 (most extreme positive)\n",
    "    The compound score is a normalized score calculated by summing the valence scores of each word in the lexicon.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    sentiment of the text: (str)\n",
    "    \"\"\"\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return scores[\"compound\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.assign(n_words=train_df[\"text\"].apply(get_length_in_words))\n",
    "train_df = train_df.assign(vader_sentiment=train_df[\"text\"].apply(get_sentiment))\n",
    "train_df = train_df.assign(rel_char_len=train_df[\"text\"].apply(get_relative_length))\n",
    "\n",
    "test_df = test_df.assign(n_words=test_df[\"text\"].apply(get_length_in_words))\n",
    "test_df = test_df.assign(vader_sentiment=test_df[\"text\"].apply(get_sentiment))\n",
    "test_df = test_df.assign(rel_char_len=test_df[\"text\"].apply(get_relative_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mention_count\n",
    "train_df['mention_count'] = train_df['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
    "test_df['mention_count'] = test_df['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
    "\n",
    "# punctuation_count\n",
    "train_df['punctuation_count'] = train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "test_df['punctuation_count'] = test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "# unique_word_count\n",
    "train_df['unique_word_count'] = train_df['text'].apply(lambda x: len(set(str(x).split())))\n",
    "test_df['unique_word_count'] = test_df['text'].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "# stop_word_count\n",
    "train_df['stop_word_count'] = train_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\n",
    "test_df['stop_word_count'] = test_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\n",
    "\n",
    "# url_count\n",
    "train_df['url_count'] = train_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "test_df['url_count'] = test_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "\n",
    "# mean_word_length\n",
    "train_df['mean_word_length'] = train_df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_df['mean_word_length'] = test_df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "# char_count\n",
    "train_df['char_count'] = train_df['text'].apply(lambda x: len(str(x)))\n",
    "test_df['char_count'] = test_df['text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# hashtag_count\n",
    "train_df['hashtag_count'] = train_df['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "test_df['hashtag_count'] = test_df['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mention count reasoning:\n",
    "In the event of a crisis, people frequently mention their loved ones and other authorities in attempt to disseminate the message.\n",
    "\n",
    "##### Punctuation count reasoning:\n",
    "When there is an emergency or a calamity, people prefer to use exclamation marks and other punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from spacymoji import Emoji\n",
    "import en_core_web_md  # pre-trained model\n",
    "import spacy\n",
    "\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(\"emoji\", first=True);\n",
    "\n",
    "def get_emoji_count(text):\n",
    "    \"\"\"\n",
    "    Returns the count of emojis in specified text\n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "    \n",
    "    Returns:\n",
    "    ------\n",
    "    count of emojis in specified text : int\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return len(doc._.emoji)\n",
    "\n",
    "train_df['emoji_count']=train_df[\"text\"].apply(get_emoji_count)\n",
    "test_df['emoji_count']=test_df[\"text\"].apply(get_emoji_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Emoji count reasoning:\n",
    "When there is an emergency or a calamity, people prefer to use less emoji's while tweeting as compared to writing a joke or a movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>n_words</th>\n",
       "      <th>vader_sentiment</th>\n",
       "      <th>rel_char_len</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>stop_word_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>char_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>emoji_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3289</th>\n",
       "      <td>debris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unfortunately, both plans fail as the 3 are im...</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.7650</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>crash</td>\n",
       "      <td>SLC</td>\n",
       "      <td>I hope this causes Bernie to crash and bern. S...</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.5697</td>\n",
       "      <td>0.267857</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>collide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>â€”pushes himself up from the chair beneath to r...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.439286</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5.888889</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9622</th>\n",
       "      <td>suicide%20bomb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Widow of CIA agent killed in 2009 Afghanistan ...</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.9460</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5.722222</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>screaming</td>\n",
       "      <td>Azania</td>\n",
       "      <td>As soon as God say yes they'll be screaming we...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>0.203571</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3.461538</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             keyword location  \\\n",
       "3289          debris      NaN   \n",
       "2672           crash      SLC   \n",
       "2436         collide      NaN   \n",
       "9622  suicide%20bomb      NaN   \n",
       "8999       screaming   Azania   \n",
       "\n",
       "                                                   text  target  n_words  \\\n",
       "3289  Unfortunately, both plans fail as the 3 are im...       0       22   \n",
       "2672  I hope this causes Bernie to crash and bern. S...       0       18   \n",
       "2436  â€”pushes himself up from the chair beneath to r...       0       21   \n",
       "9622  Widow of CIA agent killed in 2009 Afghanistan ...       1       20   \n",
       "8999  As soon as God say yes they'll be screaming we...       0       14   \n",
       "\n",
       "      vader_sentiment  rel_char_len  mention_count  punctuation_count  \\\n",
       "3289          -0.7650      0.425000              0                  2   \n",
       "2672          -0.5697      0.267857              0                  7   \n",
       "2436           0.0000      0.439286              0                  6   \n",
       "9622          -0.9460      0.428571              0                  5   \n",
       "8999           0.2960      0.203571              0                  1   \n",
       "\n",
       "      unique_word_count  stop_word_count  url_count  mean_word_length  \\\n",
       "3289                 20                7          0          5.000000   \n",
       "2672                 14                6          1          4.428571   \n",
       "2436                 18                6          1          5.888889   \n",
       "9622                 18                4          1          5.722222   \n",
       "8999                 13                5          0          3.461538   \n",
       "\n",
       "      char_count  hashtag_count  emoji_count  \n",
       "3289         119              0            0  \n",
       "2672          75              0            0  \n",
       "2436         123              0            0  \n",
       "9622         120              0            0  \n",
       "8999          57              0            2  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "drop_features = ['location']\n",
    "text_feature = \"text\" \n",
    "key_word= \"keyword\"\n",
    "target = \"target\"\n",
    "numeric_features = list(\n",
    "    set(train_df.columns)\n",
    "    - set(drop_features)\n",
    "    - set([text_feature])\n",
    "    - set([key_word])\n",
    "    -set([target])\n",
    ")\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), numeric_features),\n",
    "    (CountVectorizer(stop_words=\"english\", max_features= 15000), text_feature),\n",
    "    (CountVectorizer(stop_words=\"english\", max_features = 50), key_word)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9096 entries, 3289 to 7336\n",
      "Data columns (total 16 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   keyword            9096 non-null   object \n",
      " 1   location           6370 non-null   object \n",
      " 2   text               9096 non-null   object \n",
      " 3   target             9096 non-null   int64  \n",
      " 4   n_words            9096 non-null   int64  \n",
      " 5   vader_sentiment    9096 non-null   float64\n",
      " 6   rel_char_len       9096 non-null   float64\n",
      " 7   mention_count      9096 non-null   int64  \n",
      " 8   punctuation_count  9096 non-null   int64  \n",
      " 9   unique_word_count  9096 non-null   int64  \n",
      " 10  stop_word_count    9096 non-null   int64  \n",
      " 11  url_count          9096 non-null   int64  \n",
      " 12  mean_word_length   9096 non-null   float64\n",
      " 13  char_count         9096 non-null   int64  \n",
      " 14  hashtag_count      9096 non-null   int64  \n",
      " 15  emoji_count        9096 non-null   int64  \n",
      "dtypes: float64(3), int64(10), object(3)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_df.drop(columns=[\"target\"]), train_df[\"target\"]\n",
    "X_test, y_test = test_df.drop(columns=[\"target\"]), test_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= preprocessor.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9096, 15062)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('standardscaler',\n",
       "                                                  StandardScaler(),\n",
       "                                                  ['mention_count', 'n_words',\n",
       "                                                   'char_count',\n",
       "                                                   'stop_word_count',\n",
       "                                                   'mean_word_length',\n",
       "                                                   'unique_word_count',\n",
       "                                                   'rel_char_len', 'url_count',\n",
       "                                                   'vader_sentiment',\n",
       "                                                   'hashtag_count',\n",
       "                                                   'emoji_count',\n",
       "                                                   'punctuation_count']),\n",
       "                                                 ('countvectorizer-1',\n",
       "                                                  CountVectorizer(max_features=15000,\n",
       "                                                                  stop_words='english'),\n",
       "                                                  'text'),\n",
       "                                                 ('countvectorizer-2',\n",
       "                                                  CountVectorizer(max_features=50,\n",
       "                                                                  stop_words='english'),\n",
       "                                                  'keyword')])),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(class_weight='balanced', max_iter=2000))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr = make_pipeline(preprocessor, LogisticRegression(class_weight= 'balanced', C = 1.0, max_iter =2000))\n",
    "pipe_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dummy Classifier</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>LR_feature-engineered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>0.122 (+/- 0.004)</td>\n",
       "      <td>0.301 (+/- 0.016)</td>\n",
       "      <td>0.552 (+/- 0.079)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.053 (+/- 0.002)</td>\n",
       "      <td>0.058 (+/- 0.002)</td>\n",
       "      <td>0.063 (+/- 0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "      <td>0.811 (+/- 0.012)</td>\n",
       "      <td>0.665 (+/- 0.018)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "      <td>0.628 (+/- 0.026)</td>\n",
       "      <td>0.672 (+/- 0.022)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "      <td>0.513 (+/- 0.036)</td>\n",
       "      <td>0.678 (+/- 0.032)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_roc_auc</th>\n",
       "      <td>0.500 (+/- 0.000)</td>\n",
       "      <td>0.898 (+/- 0.011)</td>\n",
       "      <td>0.893 (+/- 0.010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_average_precision</th>\n",
       "      <td>0.187 (+/- 0.000)</td>\n",
       "      <td>0.747 (+/- 0.018)</td>\n",
       "      <td>0.737 (+/- 0.018)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Dummy Classifier Logistic Regression  \\\n",
       "fit_time                0.122 (+/- 0.004)   0.301 (+/- 0.016)   \n",
       "score_time              0.053 (+/- 0.002)   0.058 (+/- 0.002)   \n",
       "test_precision          0.000 (+/- 0.000)   0.811 (+/- 0.012)   \n",
       "test_f1                 0.000 (+/- 0.000)   0.628 (+/- 0.026)   \n",
       "test_recall             0.000 (+/- 0.000)   0.513 (+/- 0.036)   \n",
       "test_roc_auc            0.500 (+/- 0.000)   0.898 (+/- 0.011)   \n",
       "test_average_precision  0.187 (+/- 0.000)   0.747 (+/- 0.018)   \n",
       "\n",
       "                       LR_feature-engineered  \n",
       "fit_time                   0.552 (+/- 0.079)  \n",
       "score_time                 0.063 (+/- 0.003)  \n",
       "test_precision             0.665 (+/- 0.018)  \n",
       "test_f1                    0.672 (+/- 0.022)  \n",
       "test_recall                0.678 (+/- 0.032)  \n",
       "test_roc_auc               0.893 (+/- 0.010)  \n",
       "test_average_precision     0.737 (+/- 0.018)  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['LR_feature-engineered']= mean_std_cross_val_scores(pipe_lr, X_train, y_train, scoring =scoring)\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, as observed above, there is an improvement following feature engineering, i.e. the f1 score has grown dramatically after adding the additional features. In addition, while recall has grown significantly, precision has dropped. However, the model has delivered a superior f1 score, which compensates for the precision-recall tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>windstorm</th>\n",
       "      <td>2.593822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rescued</th>\n",
       "      <td>2.228706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thunderstorm</th>\n",
       "      <td>2.155966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whirlwind</th>\n",
       "      <td>1.952249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>influenza</th>\n",
       "      <td>1.936107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>died</th>\n",
       "      <td>1.927072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survived</th>\n",
       "      <td>1.919522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carried</th>\n",
       "      <td>1.881456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ukrainian</th>\n",
       "      <td>1.860744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sinkhole</th>\n",
       "      <td>1.781671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Coefficients\n",
       "windstorm         2.593822\n",
       "rescued           2.228706\n",
       "thunderstorm      2.155966\n",
       "whirlwind         1.952249\n",
       "influenza         1.936107\n",
       "died              1.927072\n",
       "survived          1.919522\n",
       "carried           1.881456\n",
       "ukrainian         1.860744\n",
       "sinkhole          1.781671"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = numeric_features + pipe_lr.named_steps[\"columntransformer\"].named_transformers_[\"countvectorizer-1\"].get_feature_names_out().tolist() + pipe_lr.named_steps[\"columntransformer\"].named_transformers_[\"countvectorizer-2\"].get_feature_names_out().tolist()\n",
    "\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "\n",
    "lr_coeffs = pipe_lr.named_steps[\"logisticregression\"].coef_\n",
    "\n",
    "cefficients= pd.DataFrame(\n",
    "    data=lr_coeffs.T,index= col_names, columns=[\"Coefficients\"]\n",
    ").sort_values(by=\"Coefficients\", ascending=False)\n",
    "\n",
    "cefficients.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the coefficients match my intuitions; as can be seen above, the attributes windstorm, rescued, thunderstorm, died, and so on have the greatest coefficients; these features reflect a disaster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dummy Classifier</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>LR_feature-engineered</th>\n",
       "      <th>CatBoost_feature-engineered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>0.122 (+/- 0.004)</td>\n",
       "      <td>0.301 (+/- 0.016)</td>\n",
       "      <td>0.552 (+/- 0.079)</td>\n",
       "      <td>13.816 (+/- 0.061)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.053 (+/- 0.002)</td>\n",
       "      <td>0.058 (+/- 0.002)</td>\n",
       "      <td>0.063 (+/- 0.003)</td>\n",
       "      <td>0.081 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "      <td>0.811 (+/- 0.012)</td>\n",
       "      <td>0.665 (+/- 0.018)</td>\n",
       "      <td>0.839 (+/- 0.025)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "      <td>0.628 (+/- 0.026)</td>\n",
       "      <td>0.672 (+/- 0.022)</td>\n",
       "      <td>0.485 (+/- 0.030)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.000 (+/- 0.000)</td>\n",
       "      <td>0.513 (+/- 0.036)</td>\n",
       "      <td>0.678 (+/- 0.032)</td>\n",
       "      <td>0.342 (+/- 0.026)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_roc_auc</th>\n",
       "      <td>0.500 (+/- 0.000)</td>\n",
       "      <td>0.898 (+/- 0.011)</td>\n",
       "      <td>0.893 (+/- 0.010)</td>\n",
       "      <td>0.851 (+/- 0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_average_precision</th>\n",
       "      <td>0.187 (+/- 0.000)</td>\n",
       "      <td>0.747 (+/- 0.018)</td>\n",
       "      <td>0.737 (+/- 0.018)</td>\n",
       "      <td>0.667 (+/- 0.015)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Dummy Classifier Logistic Regression  \\\n",
       "fit_time                0.122 (+/- 0.004)   0.301 (+/- 0.016)   \n",
       "score_time              0.053 (+/- 0.002)   0.058 (+/- 0.002)   \n",
       "test_precision          0.000 (+/- 0.000)   0.811 (+/- 0.012)   \n",
       "test_f1                 0.000 (+/- 0.000)   0.628 (+/- 0.026)   \n",
       "test_recall             0.000 (+/- 0.000)   0.513 (+/- 0.036)   \n",
       "test_roc_auc            0.500 (+/- 0.000)   0.898 (+/- 0.011)   \n",
       "test_average_precision  0.187 (+/- 0.000)   0.747 (+/- 0.018)   \n",
       "\n",
       "                       LR_feature-engineered CatBoost_feature-engineered  \n",
       "fit_time                   0.552 (+/- 0.079)          13.816 (+/- 0.061)  \n",
       "score_time                 0.063 (+/- 0.003)           0.081 (+/- 0.001)  \n",
       "test_precision             0.665 (+/- 0.018)           0.839 (+/- 0.025)  \n",
       "test_f1                    0.672 (+/- 0.022)           0.485 (+/- 0.030)  \n",
       "test_recall                0.678 (+/- 0.032)           0.342 (+/- 0.026)  \n",
       "test_roc_auc               0.893 (+/- 0.010)           0.851 (+/- 0.004)  \n",
       "test_average_precision     0.737 (+/- 0.018)           0.667 (+/- 0.015)  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "pipe_catboost_all = make_pipeline(\n",
    "    preprocessor, CatBoostClassifier(random_state=123, verbose = 0)\n",
    ")\n",
    "results['CatBoost_feature-engineered']= mean_std_cross_val_scores(pipe_catboost_all, X_train, y_train, scoring =scoring)\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score\n",
      "0.7031431897555296\n",
      "Precision Score\n",
      "0.6771300448430493\n",
      "Recall Score\n",
      "0.7312348668280871\n",
      "ROC AUC Score\n",
      "0.8269285564661661\n",
      "Average Precision Score\n",
      "0.5439537630737555\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score,average_precision_score\n",
    "\n",
    "print('F1 Score')\n",
    "print(f1_score(y_test, pipe_lr.predict(X_test)))\n",
    "\n",
    "print('Precision Score')\n",
    "print(precision_score(y_test, pipe_lr.predict(X_test)))\n",
    "\n",
    "print('Recall Score')\n",
    "print(recall_score(y_test, pipe_lr.predict(X_test)))\n",
    "\n",
    "print('ROC AUC Score')\n",
    "print(roc_auc_score(y_test, pipe_lr.predict(X_test)))\n",
    "\n",
    "print('Average Precision Score')\n",
    "print(average_precision_score(y_test, pipe_lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The f1 score and the recall scores are good for the test set also the roc -auc score looks good for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:573]",
   "language": "python",
   "name": "conda-env-573-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
